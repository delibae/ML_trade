{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"CNN_LSTM.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"mount_file_id":"1pxQSHcgfHW8LVWlnLjsWF7Pr5nfBnSH1","authorship_tag":"ABX9TyMAnvhfP2RBX+Y+fPLGbCsk"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"_-2sC4JMQ7_Z"},"source":["# MODEL"]},{"cell_type":"code","metadata":{"id":"YP_lZImIQ9jQ","colab":{"base_uri":"https://localhost:8080/","height":546},"executionInfo":{"status":"error","timestamp":1620778674772,"user_tz":-540,"elapsed":137757,"user":{"displayName":"배한진","photoUrl":"","userId":"02370233592940809626"}},"outputId":"2b9c17eb-9320-4b41-c967-b6f9531ca711"},"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.init as init\n","import matplotlib.pyplot as plt\n","from google.colab import auth\n","from google.colab import drive\n","from google.colab import files\n","import pandas as pd\n","import numpy as np\n","import torch.nn.functional as F\n","gpu = 'cuda' if torch.cuda.is_available() else 'cpu'\n","print(gpu)\n","# for reproducibility\n","torch.manual_seed(777)\n","if gpu == 'cuda':\n","    torch.cuda.manual_seed_all(777)\n","\n","\n","class CNNLSTM(nn.Module):\n","\n","    def __init__(self,input_size,hidden_size,dropout,batch_size,bidirectional = False):\n","      super(CNNLSTM, self).__init__()\n","      self.conv1 = nn.Conv2d(1, 10, 3, padding=1)\n","      self.conv2 = nn.Conv2d(10,20,2, padding=1)\n","      self.conv3 = nn.Conv2d(20,1,2,padding=1)\n","      self.pool = nn.MaxPool2d(2, stride = 1)\n","      self.input_size = input_size\n","      self.rnn = nn.LSTM(input_size -1, (input_size -1)*5, num_layers = 3, dropout=dropout, batch_first=True, bidirectional=bidirectional)\n","      self.fc1 = nn.Linear(10,20)\n","      self.fc2 = nn.Linear(20,10)\n","      self.fc3 = nn.Linear(10,3)\n","      self.batch_size = batch_size\n","\n","    def forward(self, input):\n","      output = input \n","      convolution = self.conv1(output)\n","      pooling = self.pool(convolution)\n","      convolution = self.conv2(pooling)\n","      pooling = self.pool(convolution)\n","      convolution = self.conv3(pooling)\n","      pooling = self.pool(convolution)\n","      alstm = torch.zeros(self.batch_size,1,(self.input_size -1)*5).to(gpu)\n","      for i in range(pooling.shape[1]):\n","        poolingi = pooling[:,i,:,:]\n","        lstm , _= self.rnn(poolingi)\n","        lstm = lstm[:,-1,:]\n","        alstm[:,i,:] = lstm\n","      alstm = alstm.reshape(self.batch_size,10)\n","      fc = self.fc1(alstm)\n","      fc = self.fc2(fc)\n","      final = self.fc3(fc)\n","      weight = torch.zeros(final.shape[0],final.shape[1]).to(gpu)\n","      for j in range(final.shape[0]):\n","        a = final[j,0]\n","        b = final[j,1]\n","        c = final[j,2]\n","        weight[j,0] = a*10/(a+b+c)\n","        weight[j,1] = b*10/(a+b+c)\n","        weight[j,2] = c*10/(a+b+c)\n","      final = final.mul(weight)\n","      final = F.softmax(final,dim = -1)\n","      return final\n","\n","# model = CNNLSTM(3,15,0.2,1000).to(gpu)\n","# a = torch.randn(1000,1,30,3).to(gpu)\n","# output = model(a)\n","# print(output.shape)\n","model = CNNLSTM(3,15,0.2,20000).to(gpu)\n","train_n = np.load('/content/drive/MyDrive/LSTM DATAre/train_lstm.npy')\n","volume_n = np.load('/content/drive/MyDrive/LSTM DATAre/volume_lstm.npy')\n","TF_n = np.load('/content/drive/MyDrive/LSTM DATAre/TFce1_lstm.npy')\n","traina = torch.FloatTensor(train_n)\n","train = traina[40000:60000,:,:].to(gpu)\n","train = train.reshape(20000,1,30,3).to(gpu)\n","trainv = traina[60000:,:,:].to(gpu)\n","volumea = torch.FloatTensor(volume_n)\n","volume = volumea[40000:60000,:].to(gpu)\n","volumev = volumea[60000:,:].to(gpu)\n","TFa = torch.LongTensor(TF_n)\n","TF = TFa[40000:60000].to(gpu)\n","print(TF.shape)\n","TFv = TFa[60000:].to(gpu)\n","\n","optimizer = optim.Adam(model.parameters(), lr=0.01)\n","best_model_wts = model.state_dict() \n","best_acc = 0\n","best_epoch = 0\n","l_array = []\n","a_array = []\n","criterion = nn.CrossEntropyLoss()\n","for epoch in range(250):\n","  for i in range(2):\n","    if i == 0:\n","      output = model(train)\n","      # loss = (torch.abs((output - TF))).mean()\n","      loss = criterion(output, TF)\n","      \n","      loss1 = loss.tolist()\n","      l_array.append(loss1)\n","\n","      if (epoch + 1) % 2 == 0:\n","        print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.6f}'.format(loss))\n","      \n","      optimizer.zero_grad()\n","      loss.backward()\n","      optimizer.step()\n","    else:\n","      pass\n","    \n","    # if i == 1:\n","    #   pred = model(trainv,volumev)\n","    #   accuracy = criterion(pred,TFv)\n","    #   ac1 = accuracy.tolist()\n","    #   a_array.append(ac1)\n","    #   if best_acc > accuracy:\n","    #     best_model_wts = model.state_dict()\n","    #     best_acc = accuracy\n","    #     best_ephoch = epoch\n","    #   if (epoch+1) % 10 == 0:\n","    #     print('Epoch:', '%04d' % (epoch + 1), 'accuracy =', '{:.6f}'.format(accuracy))\n","    # else:\n","    #   pass\n","\n","plt.plot(l_array, 'r')\n","plt.plot(a_array,'b')\n","plt.show()\n","model_save_name = 'CNN_LSTMn.pt'\n","path = F\"/content/drive/My Drive/{model_save_name}\" \n","torch.save(model.state_dict(), path)"],"execution_count":2,"outputs":[{"output_type":"stream","text":["cuda\n","torch.Size([20000])\n","Epoch: 0002 cost = 1.406768\n","Epoch: 0004 cost = 1.390399\n","Epoch: 0006 cost = 1.390399\n","Epoch: 0008 cost = 1.390399\n","Epoch: 0010 cost = 1.390399\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-3cce5b53a52b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m       \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m       \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m       \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    145\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rCJgEFNRSVmU","executionInfo":{"status":"ok","timestamp":1620706862498,"user_tz":-540,"elapsed":623,"user":{"displayName":"배한진","photoUrl":"","userId":"02370233592940809626"}},"outputId":"89467e63-f0ad-43e9-de6a-8390c20e856c"},"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.init as init\n","import matplotlib.pyplot as plt\n","from google.colab import auth\n","from google.colab import drive\n","from google.colab import files\n","import pandas as pd\n","import numpy as np\n","import torch.nn.functional as F\n","\n","a = torch.randn(40,1,40,3)\n","print(a.shape)\n","conv1 = nn.Conv2d(1,10,3,padding = 1)\n","conv2 = nn.Conv2d(10,20,2,padding = 1)\n","conv3 = nn.Conv2d(20,10,2,padding = 1)\n","pool = nn.MaxPool2d(2, stride = 1)\n","print(pool)\n","output = conv1(a)\n","print(output.shape)\n","output = pool(output)\n","print(output.shape)\n","output = conv2(output)\n","print(output.shape)\n","output = pool(output)\n","print(output.shape)\n","output = conv3(output)\n","print(output.shape)\n","output = pool(output)\n","print(output.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["torch.Size([40, 1, 40, 3])\n","MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\n","torch.Size([40, 10, 40, 3])\n","torch.Size([40, 10, 39, 2])\n","torch.Size([40, 20, 40, 3])\n","torch.Size([40, 20, 39, 2])\n","torch.Size([40, 10, 40, 3])\n","torch.Size([40, 10, 39, 2])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"odJLcZRXUlDj"},"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.init as init\n","import matplotlib.pyplot as plt\n","from google.colab import auth\n","from google.colab import drive\n","from google.colab import files\n","import pandas as pd\n","import numpy as np\n","import torch.nn.functional as F\n","\n","a = torch.FloatTensor([[[1,2,3],[4,5,6]],[[7,8,9],[10,11,12]]])\n","a = a.reshape(2,6)\n","print(a)\n","b = torch.randn(6,2,3)\n","print(b)\n","print(b.reshape(6,6))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zoFFLxgO6k3C"},"source":["# 새 섹션"]},{"cell_type":"code","metadata":{"id":"ozBlo27d6lME"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"M00s2S1q6lbA","outputId":"c2af1109-7849-4e85-f190-1df3dd81b162"},"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.init as init\n","import matplotlib.pyplot as plt\n","from google.colab import auth\n","from google.colab import drive\n","from google.colab import files\n","import pandas as pd\n","import numpy as np\n","import torch.nn.functional as F\n","gpu = 'cuda' if torch.cuda.is_available() else 'cpu'\n","print(gpu)\n","# for reproducibility\n","torch.manual_seed(777)\n","if gpu == 'cuda':\n","    torch.cuda.manual_seed_all(777)\n","\n","\n","class CNNLSTM(nn.Module):\n","\n","    def __init__(self,input_size,hidden_size,dropout,batch_size,bidirectional = False):\n","      super(CNNLSTM, self).__init__()\n","      self.conv1 = nn.Conv2d(1, 20, 3, padding=1)\n","      self.conv2 = nn.Conv2d(20,40,2, padding=1)\n","      self.conv3 = nn.Conv2d(40,20,2,padding=1)\n","      self.pool = nn.MaxPool2d(2, stride = 1)\n","      self.input_size = input_size\n","      self.rnn = nn.LSTM(input_size -1, (input_size -1)*5, num_layers = 3, dropout=dropout, batch_first=True, bidirectional=bidirectional)\n","      self.fc1 = nn.Linear(200,100)\n","      self.fc2 = nn.Linear(100,50)\n","      self.fc3 = nn.Linear(50,3)\n","      self.batch_size = batch_size\n","\n","    def forward(self, input):\n","      output = input \n","      convolution = self.conv1(output)\n","      pooling = self.pool(convolution)\n","      convolution = self.conv2(pooling)\n","      pooling = self.pool(convolution)\n","      convolution = self.conv3(pooling)\n","      pooling = self.pool(convolution)\n","      alstm = torch.zeros(self.batch_size,20,(self.input_size -1)*5).to(gpu)\n","      for i in range(pooling.shape[1]):\n","        poolingi = pooling[:,i,:,:]\n","        lstm , _= self.rnn(poolingi)\n","        lstm = lstm[:,-1,:]\n","        alstm[:,i,:] = lstm\n","      for k in range(self.batch_size):\n","        alstm[k,]\n","\n","      for j in range(final.shape[0]):\n","        a = final[j,0]\n","        b = final[j,1]\n","        c = final[j,2]\n","        weight[j,0] = a*10/(a+b+c)\n","        weight[j,1] = b*10/(a+b+c)\n","        weight[j,2] = c*10/(a+b+c)\n","      final = final.mul(weight)\n","      final = F.softmax(final,dim = -1)\n","      return final\n","\n","# model = CNNLSTM(3,15,0.2,1000).to(gpu)\n","# a = torch.randn(1000,1,30,3).to(gpu)\n","# output = model(a)\n","# print(output.shape)\n","model = CNNLSTM(3,15,0.2,20000).to(gpu)\n","train_n = np.load('/content/drive/MyDrive/LSTM DATAre/train_lstm.npy')\n","volume_n = np.load('/content/drive/MyDrive/LSTM DATAre/volume_lstm.npy')\n","TF_n = np.load('/content/drive/MyDrive/LSTM DATAre/TFce1_lstm.npy')\n","traina = torch.FloatTensor(train_n)\n","train = traina[40000:60000,:,:].to(gpu)\n","train = train.reshape(20000,1,30,3).to(gpu)\n","trainv = traina[60000:,:,:].to(gpu)\n","volumea = torch.FloatTensor(volume_n)\n","volume = volumea[40000:60000,:].to(gpu)\n","volumev = volumea[60000:,:].to(gpu)\n","TFa = torch.LongTensor(TF_n)\n","TF = TFa[40000:60000].to(gpu)\n","print(TF.shape)\n","TFv = TFa[60000:].to(gpu)\n","\n","optimizer = optim.Adam(model.parameters(), lr=0.01)\n","best_model_wts = model.state_dict() \n","best_acc = 0\n","best_epoch = 0\n","l_array = []\n","a_array = []\n","criterion = nn.CrossEntropyLoss()\n","for epoch in range(250):\n","  for i in range(2):\n","    if i == 0:\n","      output = model(train)\n","      # loss = (torch.abs((output - TF))).mean()\n","      loss = criterion(output, TF)\n","      \n","      loss1 = loss.tolist()\n","      l_array.append(loss1)\n","\n","      if (epoch + 1) % 2 == 0:\n","        print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.6f}'.format(loss))\n","      \n","      optimizer.zero_grad()\n","      loss.backward()\n","      optimizer.step()\n","    else:\n","      pass\n","    \n","    # if i == 1:\n","    #   pred = model(trainv,volumev)\n","    #   accuracy = criterion(pred,TFv)\n","    #   ac1 = accuracy.tolist()\n","    #   a_array.append(ac1)\n","    #   if best_acc > accuracy:\n","    #     best_model_wts = model.state_dict()\n","    #     best_acc = accuracy\n","    #     best_ephoch = epoch\n","    #   if (epoch+1) % 10 == 0:\n","    #     print('Epoch:', '%04d' % (epoch + 1), 'accuracy =', '{:.6f}'.format(accuracy))\n","    # else:\n","    #   pass\n","\n","plt.plot(l_array, 'r')\n","plt.plot(a_array,'b')\n","plt.show()\n","model_save_name = 'CNN_LSTMn.pt'\n","path = F\"/content/drive/My Drive/{model_save_name}\" \n","torch.save(model.state_dict(), path)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["cuda\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:63: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n","  \"num_layers={}\".format(dropout, num_layers))\n"],"name":"stderr"},{"output_type":"stream","text":["torch.Size([20000])\n","Epoch: 0002 cost = 0.792742\n","Epoch: 0004 cost = 0.715487\n","Epoch: 0006 cost = 0.712496\n","Epoch: 0008 cost = 0.712496\n"],"name":"stdout"}]}]}